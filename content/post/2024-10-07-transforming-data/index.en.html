---
title: Transforming data
date: '2024-10-07 14:00:00 -0800'
categories: [R, modeling]
tags: [R, rstudio] # tags always lowercase
author: Stefano
math: yes
output: 
  html_document:
    keep_md: TRUE
editor_options: 
  chunk_output_type: console
---



<!-- This blog covers data transformations in `R`. If you use `Python`, see [this post](needs-a-url). -->
<div id="transforming-response-variables" class="section level1">
<h1>Transforming response variables</h1>
<p>Data transformations are often taught in introductory quantitative and statistics courses as methods for dealing with non-Gaussian (i.e., not normal) data, whether it be bounded (e.g, strictly positive) or skewed (<a href="https://ubco-biology.github.io/BIOL202/transform.html">https://ubco-biology.github.io/BIOL202/transform.html</a>; <a href="https://ubco-biology.github.io/BIOL202/LSR.html#transform-the-data">https://ubco-biology.github.io/BIOL202/LSR.html#transform-the-data</a>; Section 13.3 in Whitlock &amp; Schluter 2020). In this post, I use simulated data to demonstrate issues that might arise with transforming data. I use a simulated dataset rather than an empirical one so we can compare the model results to the truth.</p>
<p>Imagine that a dangerous intersection has been causing frequent accidents between motored vehicles. A local group of concerned citizens decides to count the number of monthly accidents and report it to City Council. City Council decides to intervene by placing better street signs and lighting. The group of citizens then decided to count the number of accidents again to test whether or not there was an improvement in the number of accidents.</p>
<p>For the sake of this post, let’s assume that the mean number of accidents in the 12 months before the intervention was <span class="math inline">\(\lambda_0=2\)</span> per month. In the 12 months after the intervention, the mean number of monthly accidents decreases to <span class="math inline">\(\lambda_1 = 0.4\)</span>. We can then simulate data for before and after the intervention as follows:</p>
<pre class="r"><code>library(&#39;dplyr&#39;)   # for data wrangling
library(&#39;tidyr&#39;)   # for data wrangling
library(&#39;mgcv&#39;)    # for modeling
library(&#39;ggplot2&#39;) # for fancy plots
library(&#39;gratia&#39;)  # for diagnostic plots in ggplot

# change default ggplot theme
theme_set(theme_bw() +
            theme(panel.grid = element_blank(), legend.position = &#39;top&#39;,
                  text = element_text(face = &#39;bold&#39;)))

set.seed(2024+10+7) # for reproducible results
d &lt;- expand_grid(month = 1:12,
                 period = factor(c(&#39;before&#39;, &#39;after&#39;),
                                 levels = c(&#39;before&#39;, &#39;after&#39;))) %&gt;%
  mutate(lambda = if_else(period == &#39;before&#39;, true = 2, false = 0.4),
         accidents = rpois(n = n(), lambda = lambda))

ggplot(d) +
  facet_wrap(~ period, ncol = 1) +
  geom_histogram(aes(accidents), binwidth = 1, fill = &#39;grey&#39;,
                 color = &#39;black&#39;) +
  geom_vline(aes(xintercept = lambda), color = &#39;darkorange&#39;, lwd = 1) +
  labs(x = &#39;Accidents per month&#39;, y = &#39;Count&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-1-1.png" width="\linewidth" /></p>
<p>The distribution of the number of monthly accidents is clearly non-Gaussian both before and after the intervention. (<em>Also note that the number of accidents cannot be negative.</em>) Still, let’s fit a linear model to estimate the mean number of accidents in each period. In this post I use the <code>mgcv::gam()</code> function to fit all models for consistency and convenience, but note that you could also use the <code>stats::lm()</code> and <code>stats::glm()</code> functions, too. After fitting the model, we can look at the estimated change in mean accidents and appraise the model using diagnostic plots.</p>
<pre class="r"><code>m_1 &lt;- gam(accidents ~ period, data = d)
coef(m_1)</code></pre>
<pre><code>## (Intercept) periodafter 
##    1.666667   -1.250000</code></pre>
<p>From the output of the <code>coef()</code> function, we can see that the estimated model is <span class="math display">\[Y = 1.67 - 1.25\,x_1,\]</span> where <span class="math inline">\(x_1\)</span> is 0 for before the intervention and 1 for after. The <code>periodafter</code> coefficient indicates that there were -1.25 less accidents per month after the intervention. While this does not match the true effect of <span class="math inline">\(2 - 0.4 = 1.6\)</span>, we should expect some random variation, and the estimate is still reasonable. Longer observational periods would give us more accurate estimates.</p>
<pre class="r"><code>appraise(m_1, point_alpha = 0.3)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="\linewidth" /></p>
<p>Those who are used to looking at these plots may recognize two issues: (1) the residuals are not Gaussian (see the q-q plot and the histogram, in the left column), and (2) the variance in the observed values is lower when the predicted mean (linear predictor and fitted values) is lower. One may then decide to log-transform the number of accidents, after adding a small value to avoid taking the log of zero (since <span class="math inline">\(\log(0) = - \infty\)</span>).</p>
<pre class="r"><code># find proportion of months with no accidents
d %&gt;%
  group_by(period) %&gt;%
  summarize(prop_zero = mean(accidents == 0))</code></pre>
<pre><code>## # A tibble: 2 × 2
##   period prop_zero
##   &lt;fct&gt;      &lt;dbl&gt;
## 1 before     0.167
## 2 after      0.667</code></pre>
<pre class="r"><code># add a variable of log(accidents + 1)
d$log1p_accidents &lt;- log1p(d$accidents)
m_2 &lt;- gam(log1p_accidents ~ period, data = d)
coef(m_2)</code></pre>
<pre><code>## (Intercept) periodafter 
##   0.8620910  -0.5972532</code></pre>
<pre class="r"><code>appraise(m_2, point_alpha = 0.3)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-4-1.png" width="\linewidth" /></p>
<p>The <code>log1p</code> transformation resulted in a somewhat more symmetrical distribution of residuals, and the mode of the distribution is closer to zero, and improved the issue of non-constant variance (heteroskedasticity). The coefficients are also harder to interpret because they are on the <code>log1p</code> scale, since the model is <span class="math display">\[\log(Y+1)=0.86 - 0.60 \,x_1\]</span>.</p>
<p>Rather than forcing our response variable to be Gaussian when it isn’t (see <a href="https://events.ok.ubc.ca/event/fitting-models-to-data-not-data-to-models-eight-workshop-series/">this modeling workshop</a> I created with the CSC), we can fit a model that fits our data well: a Generalized Linear Model (GLM). GLMs allow one to recognize the structure of the response variable (e.g., any bounds and the mean-variance relationship) by specifying an appropriate statistical distribution (e.g., Gaussian, binomial, etc.).</p>
<p>The Poisson distribution is a good option because it represents the number of events that occur within a given period of time and location. We can fit a Poisson GLM by specifying the family in the <code>gam()</code> function:</p>
<pre class="r"><code>m_3 &lt;- gam(accidents ~ period, data = d, family = poisson(link = &#39;log&#39;))
coef(m_3)</code></pre>
<pre><code>## (Intercept) periodafter 
##   0.5108256  -1.3862944</code></pre>
<pre class="r"><code>appraise(m_3, point_alpha = 0.3)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="\linewidth" /></p>
<p>The output from <code>coef()</code> tells us that the GLM is <span class="math display">\[\lambda = e{0.51-1.39\,x_1}\]</span>. This means that after the intervention, the number of accidents decreased to <span class="math inline">\(e^-1.39 * 100%= 0.25%\)</span> what they were in the 12 months before.</p>
<p>With the GLM, we can see a improvement in both the q-q plot and histogram of residuals, and while the variance still increases with the mean number of accidents, the model now expects that to be the case. This is because the variance of a Poisson distributions is equal to its mean, both of which are usually indicated as <span class="math inline">\(\lambda\)</span> rather than <span class="math inline">\(\lambda\)</span>.</p>
<p>The <span class="math inline">\(\log\)</span> link function (<code>link = 'log'</code>) allows us to create a model that will only give us non-negative numbers of accidents by having the form <span class="math display">\[\log(\lambda) = 0.51-1.39 \, x_1,\]</span> which we can re-write as <span class="math display">\[\lambda = e^{ 0.51-1.39 \, x_1}\]</span>. Notice that while this model may look similar to the <code>log1p</code> model, the <span class="math inline">\(\log\)</span> function is applied to <span class="math inline">\(\lambda\)</span>, the mean of <span class="math inline">\(Y\)</span>, rather than to <span class="math inline">\(Y\)</span> directly. This is important because it removes the systematic bias that results from applying non-linear transformations such as <span class="math inline">\(\log(Y)\)</span>, <span class="math inline">\(\sqrt{Y}\)</span>, and <span class="math inline">\(\arcsin\left(\sqrt{Y}\right)\)</span> to a random variable. Now that we have three models, let’s compare between the predictions that the tree give us:</p>
<pre class="r"><code># predictions
pred &lt;- function(model, model_type) {
  p &lt;- predict(model, newdata = newdata, se.fit = TRUE) %&gt;%
    as.data.frame()
  
  p &lt;- p %&gt;%
    transmute(
      model = model_type,
      # get predictions and approximate 95% Bayesian Credible Intervals
      est = case_when(model == &#39;LM&#39; ~ fit,
                      grepl(&#39;log1p&#39;, model) ~ exp(fit) - 1,
                      model == &#39;Poisson GLM&#39; ~ exp(fit)),
      lwr_95 = case_when(model == &#39;LM&#39; ~ fit - 1.96 * se.fit,
                         grepl(&#39;log1p&#39;, model) ~ exp(fit - 1.96 * se.fit) - 1,
                         model == &#39;Poisson GLM&#39; ~ exp(fit - 1.96 * se.fit)),
      upr_95 = case_when(model == &#39;LM&#39; ~ fit + 1.96 * se.fit,
                         grepl(&#39;log1p&#39;, model) ~ exp(fit + 1.96 * se.fit) - 1,
                         model == &#39;Poisson GLM&#39; ~ exp(fit + 1.96 * se.fit))) %&gt;%
    bind_cols(newdata, .) %&gt;%
    mutate(x = paste(period, model))
  p
}

newdata &lt;- tibble(period = unique(d$period))
preds &lt;-
  bind_rows(
    pred(model = m_1, model_type= &#39;LM&#39;),
    pred(model = m_2, model_type= &#39;LM of log1p&#39;),
    pred(model = m_3, model_type= &#39;Poisson GLM&#39;))

ggplot() +
  facet_wrap(~ period, nrow = 1) +
  geom_hline(yintercept = 0, color = &#39;grey&#39;) +
  geom_hline(aes(yintercept = lambda), color = &#39;black&#39;, d, lty = &#39;dashed&#39;) +
  geom_jitter(aes(&#39;LM of log1p&#39;, accidents), d, alpha = 0.3, height = 0) +
  geom_errorbar(aes(model, ymin = lwr_95, ymax = upr_95,
                    color = model), preds, alpha = 0.3, width = 0) +
  geom_point(aes(model, est, color = model), preds) +
  labs(x = NULL, y = &#39;Number of accidents&#39;) +
  khroma::scale_color_highcontrast(name = &#39;Model&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="\linewidth" /></p>
<p>From the plot above, we can see that the LM and GLM have fairly similar coefficient estimates and that the <code>log1p</code> LM resulted in lower estimates for both. This is due to a systematic bias that occurs when one applies a nonlinear transformation to a random variable, like taking $$. I explain why in the sections below. Additionally, while the LM’s coefficients are reasonable, the 95% <a href="https://en.wikipedia.org/wiki/Credible_interval">Bayesian Credible Intervals</a> (BCIs) for the post-intervention estimate are not appropriate because they include negative values. In contrast, the GLM’s BCIs are asymmetric: they are shorter below the mean and longer above the mean. This is recognizes that a multiplicative change for large values is larger than one for small values: a 50% decrease will take 2 to 1 and take 4 to 2. Consequently, asymmetric BCIs provide a better representation of the model’s uncertainty.</p>
</div>
<div id="linear-and-nonlinear-transformations" class="section level1">
<h1>Linear and nonlinear transformations</h1>
<p>I want to end this section by explaining the difference between linear and nonlinear transformations and how the two impact modeling. Linear transformations include all transformations that can be written as a series of additions, subtractions, multiplications, or divisions. Formally, they have the form <span class="math inline">\(Y^* = (aY + b)\)</span>, where <span class="math inline">\(Y\)</span> is our original response variable of interest, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are any number (but <span class="math inline">\(a \ne 0\)</span>), and <span class="math inline">\(Y^*\)</span> is the transformed response. These transformations are called “linear” because the operations of addition, subtraction, multiplication and division only shift distributions left and right (addition and subtraction) or expand and shrink distributions (multiplication and division), so <span class="math inline">\(Y^*\)</span> and <span class="math inline">\(Y\)</span> follow a linear relationship for any finite and nonzero value of <span class="math inline">\(a\)</span> and any finite value of <span class="math inline">\(b\)</span>:</p>
<pre class="r"><code>Y &lt;- seq(0.01, 2, by = 1e-3)
ggplot() + geom_line(aes(Y, Y/10 + 3))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="\linewidth" /></p>
<p>Conversely, nonlinear transformations are all transformations that cannot be written using only addition, subtraction, multiplications, or division. If we were to plot any of these functions, we would see a nonlinear relationship between the original and transformed values:</p>
<pre class="r"><code>expand_grid(Y = Y,
            trans = c(&#39;Y / 10 + 3&#39;, &#39;log(Y)&#39;, &#39;sqrt(Y)&#39;,
                      &#39;arcsin(sqrt(Y))&#39;)) %&gt;%
  mutate(
    Y_star = case_when(trans == &#39;Y / 10 + 3&#39; ~ Y/10 + 3,
                       trans == &#39;log(Y)&#39; ~ log(Y),
                       trans == &#39;sqrt(Y)&#39; ~ sqrt(Y),
                       trans == &#39;arcsin(sqrt(Y))&#39; ~ asin(sqrt(Y)))) %&gt;%
  ggplot(aes(Y, Y_star)) +
  facet_wrap(~ trans, scales = &#39;free_y&#39;) +
  geom_line() +
  labs(x = &#39;Y&#39;, y = &#39;Y*&#39;)</code></pre>
<pre><code>## Warning: There was 1 warning in `mutate()`.
## ℹ In argument: `Y_star = case_when(...)`.
## Caused by warning in `asin()`:
## ! NaNs produced</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.png" width="\linewidth" /></p>
<div id="jensens-inequality" class="section level2">
<h2>Jensen’s inequality</h2>
<p><a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a> demostrates that if we apply a nonlinear transformation to a random variable, we can’t simply back-transform the mean. This is because, if <span class="math inline">\(g()\)</span> is a nonlinear transformation and <span class="math inline">\(g^{-1}()\)</span> is its inverse (e.g, <span class="math inline">\(g() = \log()\)</span> and <span class="math inline">\(g^{-1}()=\exp()\)</span>): <span class="math display">\[g\big[\mathbb E(Y)\big] \ne \mathbb E\big[g(Y)\big],\]</span> where <span class="math inline">\(\mathbb E()\)</span> indicates an expected value such that <span class="math inline">\(\mathbb E(Y)=\mu\)</span>. This implies that <span class="math display">\[\mathbb E(Y) = \mu \ne g^{-1}\bigg\{\mathbb E\big[g(Y)\big]\bigg\}\]</span>.</p>
<p>Here is a graphical example to help you visualize why. The green vertical line is the estimated mean (i.e., what we’re usually interested in), the blue line is the estimated mean of the square-root transformed <span class="math inline">\(Y\)</span>, and the blue line is the back-transformed estimated mean of the square-root transformed <span class="math inline">\(Y\)</span>. The black line is the estimated bias.</p>
<pre class="r"><code>set.seed(2024-10-7)
Y &lt;- rpois(n = 250, lambda = 1)

ggExtra::ggMarginal(
  ggplot() +
    geom_line(aes(seq(0, ceiling(max(Y)), by = 0.01),
                  y = sqrt(after_stat(x))),
              color = &#39;grey&#39;) +
    geom_jitter(aes(Y, sqrt(Y)), height = 0.02, alpha = 0.3) +
    geom_vline(aes(color = &#39;estimated~E(Y)&#39;, xintercept = mean(Y)),
               lwd = 1) +
    geom_hline(aes(color = &#39;estimated~E(sqrt(Y))&#39;,
                   yintercept = mean(sqrt(Y))), lwd = 1) +
    geom_vline(aes(color = &#39;estimated~(E(sqrt(Y)))^2&#39;,
                   xintercept = mean(sqrt(Y))^2), lwd = 1) +
    geom_segment(aes(x = mean(sqrt(Y))^2, xend = mean(Y), y = 0.5, yend = 0.5),
                 arrow = grid::arrow(ends = &#39;both&#39;, length = unit(0.1, &#39;in&#39;))) +
    labs(x = &#39;Y&#39;, y = expression(bold(sqrt(Y)))) +
    khroma::scale_color_bright(name = &#39;Variable&#39;,
                               labels = scales::parse_format()) +
    theme(legend.position = &#39;inside&#39;, legend.position.inside = c(0.8, 0.2),
          legend.frame = element_rect(fill = &#39;black&#39;)),
  fill = &#39;grey&#39;, type = &#39;histogram&#39;, xparams = list(bins = 5),
  yparams = list(bins = 5))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-9-1.png" width="\linewidth" /></p>
<p>However, note that the transformations only cause bias if applied to random variables. You can apply nonlinear transformations to predictor variables that you assume are measured exactly right. This means that fitting a model of the form <span class="math inline">\(Y = \beta_0 + \beta_1 \log(x_1)\)</span> will not cause issues if you assume <span class="math inline">\(x_1\)</span> is exactly correct with no error (as we often assume). Applying nonlinear transformations on predictors can be useful when a predictor has a distribution with a very long tail, or if one wants to create a variable with diminishing effects (e.g., moving closer to something matters more when you’re close and less when you’re already far from it). I will make a blog post on this in the future.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Whitlock M. &amp; Schluter D. (2020). Analysis of biological data, Third edition. Macmillan Learning, New York, NY.</p>
</div>
